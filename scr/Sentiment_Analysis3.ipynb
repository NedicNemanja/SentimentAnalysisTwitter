{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Sentiment Analysis_**\n",
    "\n",
    "### Here is everything you are going to need to run this notebook\n",
    "\n",
    "* nltk.download('all')\n",
    "* pip install pyenchant --user\n",
    "* pip install autocorrect --user\n",
    "* pip install sklearn --user\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some globals to help run this notebook better.\n",
    "There some files that have the results of the cleaning ready.\n",
    "They take a lot of time so you should propably run the files \n",
    "to see the results.\n",
    "\n",
    "If you want you can of course dont run the part that takes a lot of time(autocorrect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_files=1\n",
    "use_autocorrect=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first thing that we are going to do is load all the files that we need.\n",
    "\n",
    "The twitters datasets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_gold = pd.read_csv(\"../twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt\", encoding='utf8',delimiter='\\t',header=None);\n",
    "dataset_train = pd.read_csv(\"../twitter_data/train2017.tsv\", encoding='utf8',delimiter='\\t',header=None);\n",
    "dataset_test = pd.read_csv(\"../twitter_data/test2017.tsv\", encoding='utf8',delimiter='\\t',header=None);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_files import read_lexica\n",
    "all_lex=[] #list of dictionaries\n",
    "all_lex.append(read_lexica(\"../lexica/affin/affin.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/emotweet/valence_tweet.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/generic/generic.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/nrc/val.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/nrctag/val.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/sentric/senticnet5.txt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries that we are going to need below are here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))  \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from read_files import read_dict\n",
    "\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is were the cleaning is.If you want to skip this part and use the files set the use_flies variable to 1.Also if you want to use the autocorrect set the use_autocorrect variable to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdas\n"
     ]
    }
   ],
   "source": [
    "y2 = dataset_train.iloc[:, 2]\n",
    "y3 = dataset_train.iloc[:, 3]\n",
    "train_data=list(map(list, zip(y2,y3))) #list of tweets and values\n",
    "\n",
    "from fix_tweets import clean_tw\n",
    "fixed_tweets,hash_tags,name_tags=clean_tw(train_data)\n",
    "\n",
    "words=read_dict()\n",
    "\n",
    "import csv        \n",
    "print(\"asdas\")\n",
    "if use_files==0:\n",
    "    i=0\n",
    "    for tweet in fixed_tweets:\n",
    "        tw=tweet[0]\n",
    "        tw=tknzr.tokenize(tw)\n",
    "        tw1=[]\n",
    "        for word in tw:\n",
    "            word=word.lower()\n",
    "            if word not in nltk_stopwords:\n",
    "                if (not d.check(word)) and (word not in words):\n",
    "                    if use_autocorrect==1:\n",
    "                        wdd=spell(word)\n",
    "                        if wdd!=word :\n",
    "                            if word!='!':\n",
    "                                word=wdd\n",
    "\n",
    "                            tw1.append(stemmer.stem(word))\n",
    "                    else:\n",
    "                        tw1.append(stemmer.stem(word))\n",
    "\n",
    "                else:\n",
    "                    tw1.append(stemmer.stem(word))\n",
    "\n",
    "        tweet[0] = \" \".join(tw1)\n",
    "    \n",
    "    with open('../resources/twitter_fixed.tsv', 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        for tweet in fixed_tweets:\n",
    "            tsv_output.writerow([tweet[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7f9af8f137b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk_stopwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ascii\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0morder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 962\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    963\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Output array must be C or F contiguous'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1187\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_fix_train = pd.read_csv('../resources/twitter_fixed.tsv', encoding='utf8',delimiter='\\t',header=None);\n",
    "\n",
    "corpus=[]\n",
    "for i in range(dataset_fix_train.shape[0]):\n",
    "    tweet = dataset_fix_train.iloc[i, 0]\n",
    "    corpus.append(str(tweet))\n",
    "\n",
    "test_tweets=[]   \n",
    "for i in range(dataset_test.shape[0]):\n",
    "    tweet = dataset_test.iloc[i, 3]\n",
    "    test_tweets.append(tweet) \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=nltk_stopwords ,max_features=2000)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = dataset_train.iloc[:, 2]\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "X_test= vectorizer.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_test=dataset_gold.iloc[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the results of bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.30      0.38      0.33      3972\n",
      "     neutral       0.48      0.32      0.39      5937\n",
      "    positive       0.22      0.30      0.25      2375\n",
      "\n",
      "    accuracy                           0.34     12284\n",
      "   macro avg       0.33      0.33      0.32     12284\n",
      "weighted avg       0.37      0.34      0.34     12284\n",
      "\n",
      "______________________________________________________\n",
      "accuracy_score normalized\n",
      "0.3369423640507978\n",
      "_________________________\n",
      "accuracy_score\n",
      "4139\n",
      "_________________________\n",
      "[0.33466667 0.38692207 0.25156334]\n",
      "0.32438402606118083\n",
      "0.3369423640507978\n",
      "0.3438550374464943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score\n",
    "\n",
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the bag of words above we used unigrams to get those results.In sentiment analysis this is not the best approach.We will now use bigrams and see how our results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bigram = CountVectorizer(stop_words=nltk_stopwords ,max_features=2000,ngram_range=(2, 2))\n",
    "X = vectorizer_bigram.fit_transform(corpus).toarray()\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "X_test= vectorizer_bigram.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-a03b4ae56639>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-a03b4ae56639>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    The results this time\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "The results this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(\"F1_score with average [None,macro,micro,weighted]\")\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the results seem to be worse.This is not what we expected.This might be because we didnt have enought tweets to train under sets of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try one last time with trigrams so we can catch cases like \"not very good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_trigram = CountVectorizer(stop_words=nltk_stopwords ,max_features=10,ngram_range=(3, 3))\n",
    "X = vectorizer_trigram.fit_transform(corpus).toarray()\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "X_test= vectorizer_trigram.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(\"F1_score with average [None,macro,micro,weighted]\")\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=nltk_stopwords, strip_accents=\"ascii\", lowercase=True)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "X_test= vectorizer.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_test=dataset_gold.iloc[:, 1]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", strip_accents=\"ascii\", lowercase=True)\n",
    "#TODO use rafa's tokenization instead of file?\n",
    "file = open(\"/home/nem/PycharmProjects/SentimentAnalysis/twitter_data/test.tsv\")\n",
    "tfidf_matrix = vectorizer.fit_transform(file)\n",
    "file.close()\n",
    "\n",
    "return tfidf_matrix #.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Word2Vec import WordEmbeddingsVectorize\n",
    "\n",
    "WordEmbeddingsVectorize(fixed_tweets,size=300,pkl_filename\"../resources/word_embed_pickle.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
