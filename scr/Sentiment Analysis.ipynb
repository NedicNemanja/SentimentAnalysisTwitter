{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Sentiment Analysis_**\n",
    "\n",
    "### Here is everything you are going to need to run this notebook\n",
    "\n",
    "* nltk.download('all')\n",
    "* pip install pyenchant \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There some files that have the results of the cleaning ready.If you want to use them leave the below variable as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_files=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first thing that we are going to do is load all the files that we need.\n",
    "\n",
    "The twitters datasets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_gold = pd.read_csv(\"../twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt\", encoding='utf8',delimiter='\\t',header=None);\n",
    "dataset_train = pd.read_csv(\"../twitter_data/train2017.tsv\", encoding='utf8',delimiter='\\t',header=None);\n",
    "dataset_test = pd.read_csv(\"../twitter_data/test2017.tsv\", encoding='utf8',delimiter='\\t',header=None);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_files import read_lexica\n",
    "all_lex=[] #list of dictionaries\n",
    "all_lex.append(read_lexica(\"../lexica/affin/affin.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/emotweet/valence_tweet.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/generic/generic.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/nrc/val.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/nrctag/val.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/sentric/senticnet5.txt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries that we are going to need below are here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))  \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from read_files import read_dict\n",
    "from fix_tweets import clean_tw\n",
    "\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from os import path ,getcwd\n",
    "from wordcloud import WordCloud\n",
    "from colour_wc import random_color_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is were the cleaning is.If you want to skip this part and use the files set the use_flies variable to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = dataset_train.iloc[:, 2]\n",
    "y3 = dataset_train.iloc[:, 3]\n",
    "train_data=list(map(list, zip(y2,y3))) #list of tweets and values\n",
    "\n",
    "fixed_tweets,hash_tags,name_tags=clean_tw(train_data)\n",
    "\n",
    "words=read_dict()\n",
    "import csv        \n",
    "if use_files==0:\n",
    "    for tweet in fixed_tweets:\n",
    "        tw=tweet[0]\n",
    "        tw=tknzr.tokenize(tw)\n",
    "        tw1=[]\n",
    "        for word in tw:\n",
    "            word=word.lower()\n",
    "            tw1.append(stemmer.stem(word))\n",
    "\n",
    "        tweet[0] = \" \".join(tw1)\n",
    "    \n",
    "    with open('resources/twitter_fixed.tsv', 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        for tweet in fixed_tweets:\n",
    "            tsv_output.writerow([tweet[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fix_train = pd.read_csv('resources/twitter_fixed.tsv', encoding='utf8',delimiter='\\t',header=None);\n",
    "\n",
    "corpus=[]\n",
    "for i in range(dataset_fix_train.shape[0]):\n",
    "    tweet = dataset_fix_train.iloc[i, 0]\n",
    "    corpus.append(str(tweet))\n",
    "\n",
    "test_tweets=[]   \n",
    "for i in range(dataset_test.shape[0]):\n",
    "    tweet = dataset_test.iloc[i, 3]\n",
    "    test_tweets.append(tweet) \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=nltk_stopwords ,max_features=1000)\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = dataset_train.iloc[:, 2]\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "with open('bof_uni.pkl', 'wb') as handle:\n",
    "    pickle.dump(classifier, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('bof_uni.pkl', 'rb') as handle:\n",
    "    classifier = pickle.load(handle)\n",
    "    \n",
    "X_test= vectorizer.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_test=dataset_gold.iloc[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the results of bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the bag of words above we used unigrams to get those results.In sentiment analysis this is not the best approach.We will now use bigrams and see how our results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bigram = CountVectorizer(stop_words=nltk_stopwords ,max_features=1000,ngram_range=(2, 2))\n",
    "X = vectorizer_bigram.fit_transform(corpus).toarray()\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "with open('bof_bi.pkl', 'wb') as handle:\n",
    "    pickle.dump(classifier, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('bof_bi.pkl', 'rb') as handle:\n",
    "    classifier = pickle.load(handle)\n",
    "\n",
    "X_test= vectorizer_bigram.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The results this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(\"F1_score with average [None,macro,micro,weighted]\")\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the results seem to be worse.This is not what we expected.This might be because we didnt have enought tweets to train under sets of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try one last time with trigrams so we can catch cases like \"not very good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_trigram = CountVectorizer(stop_words=nltk_stopwords ,max_features=1000,ngram_range=(3, 3))\n",
    "X = vectorizer_trigram.fit_transform(corpus).toarray()\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "with open('bof_tri.pkl', 'wb') as handle:\n",
    "    pickle.dump(classifier, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('bof_tri.pkl', 'rb') as handle:\n",
    "    classifier = pickle.load(handle)\n",
    "\n",
    "X_test= vectorizer_trigram.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(\"F1_score with average [None,macro,micro,weighted]\")\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better than bigram but still worse than unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=nltk_stopwords, strip_accents=\"ascii\", lowercase=True,max_features=1000)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = dataset_train.iloc[:, 2]\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "X_test= vectorizer.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_test=dataset_gold.iloc[:, 1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of Tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we will try to usr bigrams to see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=nltk_stopwords, strip_accents=\"ascii\", lowercase=True,max_features=1000,ngram_range=(2, 2))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = dataset_train.iloc[:, 2]\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "X_test= vectorizer.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_test=dataset_gold.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=nltk_stopwords, strip_accents=\"ascii\", lowercase=True,max_features=1000,ngram_range=(3, 3))\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = dataset_train.iloc[:, 2]\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "X_test= vectorizer.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_test=dataset_gold.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Word2Vec import WordEmbeddingsVectorize\n",
    "\n",
    "WordEmbeddingsVectorize(fixed_tweets,size=300,pkl_filename\"resources/word_embed_pickle.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the test file to a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2=dataset_gold.iloc[:, 1]\n",
    "y3 = dataset_test.iloc[:, 3]\n",
    "test_data=list(map(list, zip(y2,y3))) \n",
    "  \n",
    "fixed_tweets,hash_tags,name_tags=clean_tw(train_data)\n",
    "fixed_tweets1,hash_tags1,name_tags1=clean_tw(test_data)\n",
    "\n",
    "fixed_tweets+=fixed_tweets1\n",
    "hash_tags+=hash_tags1\n",
    "name_tags+=name_tags1\n",
    "all_data=train_data+test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used words in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_text=\"\"\n",
    "\n",
    "for tweet_info in fixed_tweets:\n",
    "    tweet_info[0]=tknzr.tokenize(tweet_info[0].lower())\n",
    "    wd_text+=\" \".join(tweet_info[0])\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else getcwd()\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_general_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used words in positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_text=\"\"\n",
    "\n",
    "for i in range(len(fixed_tweets)):\n",
    "    if all_data[i][0]==\"positive\":       \n",
    "        wd_text+=\" \".join(fixed_tweets[i][0])\n",
    "\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_positive_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used words in negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_text=\"\"\n",
    "\n",
    "for i in range(len(fixed_tweets)):\n",
    "    if all_data[i][0]==\"negative\":       \n",
    "        wd_text+=\" \".join(fixed_tweets[i][0])\n",
    "\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_negative_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used words in neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_text=\"\"\n",
    "\n",
    "for i in range(len(fixed_tweets)):\n",
    "    if all_data[i][0]==\"neutral\":       \n",
    "        wd_text+=\" \".join(fixed_tweets[i][0])\n",
    "\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_neutral_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wd_text=\"\"\n",
    "wd_text=\" \".join(name_tags)\n",
    "\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_tags_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used hash tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_text=\"\"\n",
    "wd_text=\" \".join(hash_tags)\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_hash_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_tweets_num=len(fixed_tweets)\n",
    "all_tweets_name_tagg=0\n",
    "all_tweets_hash_tagg=0\n",
    "all_tweets_link=0\n",
    "\n",
    "for tweet in fixed_tweets:\n",
    "    if len(tweet[1])!=0:\n",
    "        all_tweets_hash_tagg+=1\n",
    "    if len(tweet[2])!=0:\n",
    "        all_tweets_name_tagg+=1   \n",
    "    if len(tweet[3])!=0:\n",
    "        all_tweets_link+=1   \n",
    "\n",
    "        \n",
    "print(\"Out of the \" +str(all_tweets_num)+\" tweets \"+str(all_tweets_hash_tagg)\n",
    "      +\" used hash tags \"+str(all_tweets_name_tagg)+\n",
    "      \" tagged someone and \"+str(all_tweets_name_tagg)+\" used links\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we try to find mistakes in tweets.This is not very accurate around (60% accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fixed_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a40329769152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmistakes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfixed_tweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtknzr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fixed_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "mistakes=0\n",
    "for tweet in fixed_tweets:\n",
    "    tw=tknzr.tokenize(tweet[0])\n",
    "    for word in tw:\n",
    "        wd=word.lower()\n",
    "        if not d.check(wd):\n",
    "            if wd not in words:\n",
    "                mistakes+=1\n",
    "                break\n",
    "print(\"Out of the \" +str(all_tweets_num)+\" tweets \"+str(mistakes)+ \" had mistakes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
