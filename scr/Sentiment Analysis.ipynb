{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Sentiment Analysis_**\n",
    "\n",
    "### Here is everything you are going to need to run this notebook\n",
    "\n",
    "* nltk.download('all')\n",
    "* pip install pyenchant \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There some files that have the results of the cleaning ready.If you want to use them leave the below variable as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_files=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first thing that we are going to do is load all the files that we need.\n",
    "\n",
    "The twitters datasets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_gold = pd.read_csv(\"../twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt\", encoding='utf8',delimiter='\\t',header=None);\n",
    "dataset_train = pd.read_csv(\"../twitter_data/train2017.tsv\", encoding='utf8',delimiter='\\t',header=None);\n",
    "dataset_test = pd.read_csv(\"../twitter_data/test2017.tsv\", encoding='utf8',delimiter='\\t',header=None);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_files import read_lexica\n",
    "all_lex=[] #list of dictionaries\n",
    "all_lex.append(read_lexica(\"../lexica/affin/affin.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/emotweet/valence_tweet.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/generic/generic.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/nrc/val.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/nrctag/val.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/sentric/senticnet5.txt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries that we are going to need below are here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))  \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from read_files import read_dict\n",
    "from fix_tweets import clean_tw\n",
    "\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "import pickle\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from os import path ,getcwd\n",
    "from wordcloud import WordCloud\n",
    "from colour_wc import random_color_func\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is were the cleaning is.If you want to skip this part and use the files set the use_flies variable to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = dataset_train.iloc[:, 2]\n",
    "y3 = dataset_train.iloc[:, 3]\n",
    "train_data=list(map(list, zip(y2,y3))) #list of tweets and values\n",
    "\n",
    "fixed_tweets,hash_tags,name_tags=clean_tw(train_data)\n",
    "\n",
    "words=read_dict()\n",
    "import csv        \n",
    "if use_files==0:\n",
    "    for tweet in fixed_tweets:\n",
    "        tw=tweet[0]\n",
    "        tw=tknzr.tokenize(tw)\n",
    "        tw1=[]\n",
    "        for word in tw:\n",
    "            word=word.lower()\n",
    "            tw1.append(stemmer.stem(word))\n",
    "\n",
    "        tweet[0] = \" \".join(tw1)\n",
    "    \n",
    "    with open('../resources/twitter_fixed.tsv', 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        for tweet in fixed_tweets:\n",
    "            tsv_output.writerow([tweet[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-5000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-4ae65a7590d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bof_uni.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -5000"
     ]
    }
   ],
   "source": [
    "dataset_fix_train = pd.read_csv('../resources/twitter_fixed.tsv', encoding='utf8',delimiter='\\t',header=None);\n",
    "\n",
    "corpus=[]\n",
    "for i in range(dataset_fix_train.shape[0]):\n",
    "    tweet = dataset_fix_train.iloc[i, 0]\n",
    "    corpus.append(str(tweet))\n",
    "\n",
    "test_tweets=[]   \n",
    "for i in range(dataset_test.shape[0]):\n",
    "    tweet = dataset_test.iloc[i, 3]\n",
    "    test_tweets.append(tweet) \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=nltk_stopwords ,max_features=1000)\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = dataset_train.iloc[:, 2]\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "with open('bof_uni.pkl', 'wb') as handle:\n",
    "    pickle.dump(classifier, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('bof_uni.pkl', 'rb') as handle:\n",
    "    classifier = pickle.load(handle)\n",
    "    \n",
    "X_test= vectorizer.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_test=dataset_gold.iloc[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the results of bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the bag of words above we used unigrams to get those results.In sentiment analysis this is not the best approach.We will now use bigrams and see how our results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bigram = CountVectorizer(stop_words=nltk_stopwords ,max_features=1000,ngram_range=(2, 2))\n",
    "X = vectorizer_bigram.fit_transform(corpus).toarray()\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "with open('bof_bi.pkl', 'wb') as handle:\n",
    "    pickle.dump(classifier, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('bof_bi.pkl', 'rb') as handle:\n",
    "    classifier = pickle.load(handle)\n",
    "\n",
    "X_test= vectorizer_bigram.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The results this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(\"F1_score with average [None,macro,micro,weighted]\")\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the results seem to be worse.This is not what we expected.This might be because we didnt have enought tweets to train under sets of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try one last time with trigrams so we can catch cases like \"not very good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_trigram = CountVectorizer(stop_words=nltk_stopwords ,max_features=1000,ngram_range=(3, 3))\n",
    "X = vectorizer_trigram.fit_transform(corpus).toarray()\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "with open('bof_tri.pkl', 'wb') as handle:\n",
    "    pickle.dump(classifier, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('bof_tri.pkl', 'rb') as handle:\n",
    "    classifier = pickle.load(handle)\n",
    "\n",
    "X_test= vectorizer_trigram.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(\"F1_score with average [None,macro,micro,weighted]\")\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is better than bigram but still worse than unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the test file to a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2=dataset_gold.iloc[:, 1]\n",
    "y3 = dataset_test.iloc[:, 3]\n",
    "test_data=list(map(list, zip(y2,y3))) \n",
    "  \n",
    "fixed_tweets,hash_tags,name_tags=clean_tw(train_data)\n",
    "fixed_tweets1,hash_tags1,name_tags1=clean_tw(test_data)\n",
    "\n",
    "fixed_tweets+=fixed_tweets1\n",
    "hash_tags+=hash_tags1\n",
    "name_tags+=name_tags1\n",
    "all_data=train_data+test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used words in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_text=\"\"\n",
    "\n",
    "for tweet_info in fixed_tweets:\n",
    "    tweet_info[0]=tknzr.tokenize(tweet_info[0].lower())\n",
    "    wd_text+=\" \".join(tweet_info[0])\n",
    "\n",
    "d = path.dirname(__file__) if \"__file__\" in locals() else getcwd()\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_general_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used words in positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_text=\"\"\n",
    "\n",
    "for i in range(len(fixed_tweets)):\n",
    "    if all_data[i][0]==\"positive\":       \n",
    "        wd_text+=\" \".join(fixed_tweets[i][0])\n",
    "\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_positive_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used words in negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_text=\"\"\n",
    "\n",
    "for i in range(len(fixed_tweets)):\n",
    "    if all_data[i][0]==\"negative\":       \n",
    "        wd_text+=\" \".join(fixed_tweets[i][0])\n",
    "\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_negative_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used words in neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_text=\"\"\n",
    "\n",
    "for i in range(len(fixed_tweets)):\n",
    "    if all_data[i][0]==\"neutral\":       \n",
    "        wd_text+=\" \".join(fixed_tweets[i][0])\n",
    "\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_neutral_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wd_text=\"\"\n",
    "wd_text=\" \".join(name_tags)\n",
    "\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_tags_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most used hash tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd_text=\"\"\n",
    "wd_text=\" \".join(hash_tags)\n",
    "mask = np.array(Image.open(path.join(d, \"twitter_mask.png\")))\n",
    "\n",
    "nltk_WC = WordCloud(background_color=\"white\",max_words=500, mask=mask, stopwords=nltk_stopwords, margin=10,\n",
    "               random_state=1).generate(wd_text)\n",
    "\n",
    "plt.figure(figsize=[7,7])\n",
    "plt.imshow(nltk_WC.recolor(color_func=random_color_func, random_state=3),\n",
    "           interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "nltk_WC.to_file(\"twitter_hash_WC.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_tweets_num=len(fixed_tweets)\n",
    "all_tweets_name_tagg=0\n",
    "all_tweets_hash_tagg=0\n",
    "all_tweets_link=0\n",
    "\n",
    "for tweet in fixed_tweets:\n",
    "    if len(tweet[1])!=0:\n",
    "        all_tweets_hash_tagg+=1\n",
    "    if len(tweet[2])!=0:\n",
    "        all_tweets_name_tagg+=1   \n",
    "    if len(tweet[3])!=0:\n",
    "        all_tweets_link+=1   \n",
    "\n",
    "        \n",
    "print(\"Out of the \" +str(all_tweets_num)+\" tweets \"+str(all_tweets_hash_tagg)\n",
    "      +\" used hash tags \"+str(all_tweets_name_tagg)+\n",
    "      \" tagged someone and \"+str(all_tweets_name_tagg)+\" used links\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
