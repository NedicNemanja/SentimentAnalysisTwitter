{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **_Sentiment Analysis_**\n",
    "\n",
    "### Here is everything you are going to need to run this notebook\n",
    "\n",
    "* nltk.download('all')\n",
    "* pip install pyenchant \n",
    "* pip install autocorrect\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have some globals to help run this notebook better.\n",
    "There some files that have the results of the cleaning ready.\n",
    "They take a lot of time so you should propably run the files \n",
    "to see the results.\n",
    "\n",
    "If you want you can of course dont run the part that takes a lot of time(autocorrect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_files=1\n",
    "use_autocorrect=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The first thing that we are going to do is load all the files that we need.\n",
    "\n",
    "The twitters datasets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_gold = pd.read_csv(\"../twitter_data/SemEval2017_task4_subtaskA_test_english_gold.txt\", encoding='utf8',delimiter='\\t',header=None);\n",
    "dataset_train = pd.read_csv(\"../twitter_data/train2017.tsv\", encoding='utf8',delimiter='\\t',header=None);\n",
    "dataset_test = pd.read_csv(\"../twitter_data/test2017.tsv\", encoding='utf8',delimiter='\\t',header=None);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_files import read_lexica\n",
    "all_lex=[] #list of dictionaries\n",
    "all_lex.append(read_lexica(\"../lexica/affin/affin.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/emotweet/valence_tweet.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/generic/generic.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/nrc/val.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/nrctag/val.txt\"))\n",
    "all_lex.append(read_lexica(\"../lexica/sentric/senticnet5.txt\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries that we are going to need below are here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk_stopwords = set(stopwords.words('english'))  \n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "from read_files import read_dict\n",
    "\n",
    "import enchant\n",
    "d = enchant.Dict(\"en_US\")\n",
    "from autocorrect import spell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is were the cleaning is.If you want to skip this part and use the files set the use_flies variable to 1.Also if you want to use the autocorrect set the use_autocorrect variable to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = dataset_train.iloc[:, 2]\n",
    "y3 = dataset_train.iloc[:, 3]\n",
    "train_data=list(map(list, zip(y2,y3))) #list of tweets and values\n",
    "\n",
    "from fix_tweets import clean_tw\n",
    "fixed_tweets,hash_tags,name_tags=clean_tw(train_data)\n",
    "\n",
    "words=read_dict()\n",
    "\n",
    "import csv        \n",
    "print(\"asdas\")\n",
    "if use_files==0:\n",
    "    i=0\n",
    "    for tweet in fixed_tweets:\n",
    "        tw=tweet[0]\n",
    "        tw=tknzr.tokenize(tw)\n",
    "        tw1=[]\n",
    "        for word in tw:\n",
    "            word=word.lower()\n",
    "            if word not in nltk_stopwords:\n",
    "                if (not d.check(word)) and (word not in words):\n",
    "                    if use_autocorrect==1:\n",
    "                        wdd=spell(word)\n",
    "                        if wdd!=word :\n",
    "                            if word!='!':\n",
    "                                word=wdd\n",
    "\n",
    "                            tw1.append(stemmer.stem(word))\n",
    "                    else:\n",
    "                        tw1.append(stemmer.stem(word))\n",
    "\n",
    "                else:\n",
    "                    tw1.append(stemmer.stem(word))\n",
    "\n",
    "        tweet[0] = \" \".join(tw1)\n",
    "    \n",
    "    with open('../resources/twitter_fixed.tsv', 'w', newline='') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\t')\n",
    "        for tweet in fixed_tweets:\n",
    "            tsv_output.writerow([tweet[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fix_train = pd.read_csv('../resources/twitter_fixed.tsv', encoding='utf8',delimiter='\\t',header=None);\n",
    "\n",
    "corpus=[]\n",
    "for i in range(dataset_fix_train.shape[0]):\n",
    "    tweet = dataset_fix_train.iloc[i, 0]\n",
    "    corpus.append(str(tweet))\n",
    "\n",
    "test_tweets=[]   \n",
    "for i in range(dataset_test.shape[0]):\n",
    "    tweet = dataset_test.iloc[i, 3]\n",
    "    test_tweets.append(tweet) \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=nltk_stopwords ,max_features=2000)\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = dataset_train.iloc[:, 2]\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "X_test= vectorizer.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_test=dataset_gold.iloc[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the results of bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.30      0.38      0.33      3972\n",
      "     neutral       0.48      0.32      0.39      5937\n",
      "    positive       0.22      0.30      0.25      2375\n",
      "\n",
      "    accuracy                           0.34     12284\n",
      "   macro avg       0.33      0.33      0.32     12284\n",
      "weighted avg       0.37      0.34      0.34     12284\n",
      "\n",
      "______________________________________________________\n",
      "accuracy_score normalized\n",
      "0.3369423640507978\n",
      "_________________________\n",
      "accuracy_score\n",
      "4139\n",
      "_________________________\n",
      "[0.33466667 0.38692207 0.25156334]\n",
      "0.32438402606118083\n",
      "0.3369423640507978\n",
      "0.3438550374464943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score\n",
    "\n",
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the bag of words above we used unigrams to get those results.In sentiment analysis this is not the best approach.We will now use bigrams and see how our results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_bigram = CountVectorizer(stop_words=nltk_stopwords ,max_features=2000,ngram_range=(2, 2))\n",
    "X = vectorizer_bigram.fit_transform(corpus).toarray()\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "X_test= vectorizer_bigram.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The results this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.32      0.91      0.48      3972\n",
      "     neutral       0.46      0.05      0.09      5937\n",
      "    positive       0.19      0.04      0.07      2375\n",
      "\n",
      "    accuracy                           0.33     12284\n",
      "   macro avg       0.33      0.33      0.21     12284\n",
      "weighted avg       0.36      0.33      0.21     12284\n",
      "\n",
      "______________________________________________________\n",
      "accuracy_score normalized\n",
      "0.3260338651904917\n",
      "_________________________\n",
      "accuracy_score\n",
      "4005\n",
      "_________________________\n",
      "F1_score with average [None,macro,micro,weighted]\n",
      "[0.47792655 0.09099181 0.06925208]\n",
      "0.21272348090689275\n",
      "0.3260338651904917\n",
      "0.21190299077806654\n"
     ]
    }
   ],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(\"F1_score with average [None,macro,micro,weighted]\")\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the results seem to be worse.This is not what we expected.This might be because we didnt have enought tweets to train under sets of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try one last time with trigrams so we can catch cases like \"not very good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_trigram = CountVectorizer(stop_words=nltk_stopwords ,max_features=10,ngram_range=(3, 3))\n",
    "X = vectorizer_trigram.fit_transform(corpus).toarray()\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X, y)\n",
    "\n",
    "X_test= vectorizer_trigram.fit_transform(test_tweets).toarray()\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.59      0.01      0.01      3972\n",
      "     neutral       0.49      0.97      0.65      5937\n",
      "    positive       0.32      0.04      0.08      2375\n",
      "\n",
      "    accuracy                           0.48     12284\n",
      "   macro avg       0.46      0.34      0.25     12284\n",
      "weighted avg       0.49      0.48      0.33     12284\n",
      "\n",
      "______________________________________________________\n",
      "accuracy_score normalized\n",
      "0.4814392705958971\n",
      "_________________________\n",
      "accuracy_score\n",
      "5914\n",
      "_________________________\n",
      "F1_score with average [None,macro,micro,weighted]\n",
      "[0.01491424 0.64787627 0.07632456]\n",
      "0.2463716941837368\n",
      "0.4814392705958971\n",
      "0.3327052798856033\n"
     ]
    }
   ],
   "source": [
    "print(\"classification_report\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"______________________________________________________\")\n",
    "\n",
    "print(\"accuracy_score normalized\")\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(\"_________________________\")\n",
    "print(\"accuracy_score\" )\n",
    "print(accuracy_score(y_test, y_pred, normalize=False))\n",
    "print(\"_________________________\")\n",
    "\n",
    "print(\"F1_score with average [None,macro,micro,weighted]\")\n",
    "print(f1_score(y_test, y_pred,average=None))\n",
    "print(f1_score(y_test, y_pred,average='macro'))\n",
    "print(f1_score(y_test, y_pred,average='micro'))\n",
    "print(f1_score(y_test, y_pred,average='weighted'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
