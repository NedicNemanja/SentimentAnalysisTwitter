{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the cell below if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rafael/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rafael/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "## _First step_\n",
    "\n",
    "First we take the data from the lexica folder and the training data from the twitter_data folder,then we hava to clean the data.We romove symbols like (#,@) and links but we keep emoticons and emojis because they convey emotion (valence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Na doume ta stop words mallon prpei na knoume lista \n",
    "#h kai na mhn ta aferesoume katholou \n",
    "#afou logika den tha uparxoun sta set me \n",
    "#tis lekseis pou mas dinoun\n",
    "import read_files\n",
    "lex = read_files.read_lexica(\"../lexica/affin/affin.txt\")\n",
    "train =read_files.read_tsv(\"../twitter_data/train2017.tsv\")\n",
    "\n",
    "import fix_tweets\n",
    "\n",
    "symbols=\"#@\"\n",
    "fix_tweets.remove_chars(train,symbols)\n",
    "fix_tweets.remove_links(train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we continue with the stop words,tokenization and stemming.The stop words come from the nltk tool kit.Symbols and links were removed from the tweets,but the stopwords are passed after the tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for tweet in train:\n",
    "    tweet[1]=word_tokenize(tweet[1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
